{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKBL6haPJNKC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install openprompt\n",
        "from datasets import load_dataset, Dataset\n",
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts import MixedTemplate, SoftVerbalizer, ManualVerbalizer\n",
        "from openprompt.data_utils import InputExample\n",
        "from openprompt import PromptDataLoader, PromptForClassification\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOsn2CWwJNKG"
      },
      "outputs": [],
      "source": [
        "rawdata = load_dataset(\"csv\", data_files={'train': [\"../data/corpus_train.csv\"], 'validation': [\"../data/corpus_valid.csv\"], 'test':[\"../data/corpus_test.csv\"]})\n",
        "rawdata = rawdata.filter(lambda example: example['label']!='title') \n",
        "rawdata = rawdata.filter(lambda example: example['label']!='common-ground') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU0zEBH_JNKH"
      },
      "outputs": [],
      "source": [
        "# load the plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cDY-nqYJNKI"
      },
      "outputs": [],
      "source": [
        "# construct a template\n",
        "\n",
        "#template_text = '{\"placeholder\": \"text_a\"} {\"soft\": \"In this sentence, the topic is\"} {\"mask\"} {\"soft\"}.'\n",
        "#{\"placeholder\": \"text_a\"} {\"placeholder\": \"text_b\"} {\"soft\":\"This\"} topic {\"soft\":\"is about\"} {\"mask\"}.\n",
        "#template_text = '{\"placeholder\":\"text_a\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"soft\"} {\"mask\"}.'\n",
        "#template_text = 'In this argumentative text with the title {\"meta\": \"title\", \"shortenable\": False}, the role of this sentence: {\"meta\": \"sentence\", \"shortenable\": False}, is {\"mask\"}.'\n",
        "#template_text = 'In an argumentative text, the role of this sentence: {\"placeholder\": \"text_a\"}, is {\"mask\"}.'\n",
        "template_text = '{\"placeholder\":\"text_a\"} {\"soft\": None, \"duplicate\": 20} {\"mask\"}.' #optimal one\n",
        "mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text=template_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHtugMqBJNKJ"
      },
      "outputs": [],
      "source": [
        "#title and common-ground are removed\n",
        "def int_label(label):\n",
        "    if label == \"assumption\": return 0\n",
        "    elif label == \"testimony\": return 1\n",
        "    elif label == \"anecdote\": return 2\n",
        "    elif label == \"statistics\": return 3\n",
        "    #elif label == \"title\": return 4\n",
        "    #elif label == \"common-ground\": return 4\n",
        "    elif label == \"other\": return 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHycZAiJJNKK"
      },
      "outputs": [],
      "source": [
        "# convert our raw data to openprompt's form\n",
        "dataset = {}\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    dataset[split] = []\n",
        "    for data in rawdata[split]:\n",
        "        #input_example = InputExample(meta={\"sentence\": data['sentence']}, label = int_label(data['label']), guid=data['article_id'])\n",
        "        input_example = InputExample(text_a= data['sentence'], label = int_label(data['label']), guid=data['article_id'])\n",
        "        dataset[split].append(input_example)\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip0eHlPfJNKO"
      },
      "outputs": [],
      "source": [
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AsYhWN4JNKP"
      },
      "outputs": [],
      "source": [
        "myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_OVcuN5JNKQ"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
        "if use_cuda: prompt_model=  prompt_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORbibrODJNKQ"
      },
      "outputs": [],
      "source": [
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# set no decay to biase and LayerNorm parameters\n",
        "optimizer_grouped_parameters1 = [\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "# different optimizer for prompt and model \n",
        "optimizer_grouped_parameters2 = [\n",
        "    {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
        "    {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
        "]\n",
        "\n",
        "\n",
        "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "optimizer2 = AdamW(optimizer_grouped_parameters2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w4i-oahJNKR"
      },
      "outputs": [],
      "source": [
        "for epoch in range(5):\n",
        "    tot_loss = 0\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        optimizer2.step()\n",
        "        optimizer2.zero_grad()\n",
        "        if step%100 ==0: \n",
        "          print(\"epoch = {}, step = {}, tot_loss/(step+1) = {}\".format(epoch, step, tot_loss/(step+1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## evaluate\n",
        "\n",
        "# %%\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "\n",
        "prompt_model.eval()\n",
        "\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(validation_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(\"validation:\",acc)\n",
        "\n",
        "print(classification_report(alllabels, allpreds, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZckvDgYlJ-8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validation:\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(\"test:\", acc)  # roughly ~0.85\n",
        "print(classification_report(alllabels, allpreds, zero_division=0))"
      ],
      "metadata": {
        "id": "sP7xUI0dcIvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawcmv = load_dataset(\"csv\", data_files={'CMV': [\"../data/cmv_train.csv\"]})\n",
        "cmvdata = []\n",
        "for data in rawcmv['CMV']:\n",
        "    input_example = InputExample(text_a= data['sentence'], label = int_label(data['label']))\n",
        "    cmvdata.append(input_example)\n",
        "print(cmvdata[0])"
      ],
      "metadata": {
        "id": "YOpY9Zwhkbfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test on CMV data:\n",
        "cmv_dataloader = PromptDataLoader(dataset= cmvdata, template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
        "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"head\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "for step, inputs in enumerate(cmv_dataloader):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "print(\"test:\", acc)  # roughly ~0.85\n",
        "print(classification_report(alllabels, allpreds, zero_division=0))"
      ],
      "metadata": {
        "id": "rq9g1ydSW116"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix(alllabels, allpreds))\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "br2hV7PBvZBo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Openprompt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}