{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2c05e3db047a88f3\n",
      "Reusing dataset csv (/home/georg/.cache/huggingface/datasets/csv/default-2c05e3db047a88f3/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c7141970dd45e8bb08d54ddee1e49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"csv\", data_files={'train': [\"../data/corpus_train.csv\"], 'validation': [\"../data/corpus_valid.csv\"], \"test\": [\"../data/corpus_test.csv\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11f5925c1f74fa6998ebaea0b8c607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9505 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_templates = [\"[INP]. This should be [MASK] knowlege\",\n",
    "                    \"[INP]. This sentence contains [MASK] proof and results.\",\n",
    "                    \"[INP]. This sentence contains a statement from a expert, authority, witness, group, organisation or similar? [MASK].\",\n",
    "                    \"[INP]. This is something I have [MASK] from personal experience.\",\n",
    "                    \"[INP]. That is a cool [MASK].\"]\n",
    "\n",
    "def apply_promts(row):\n",
    "    answers = []\n",
    "    for prompt_template in prompt_templates:\n",
    "        # replace [INP] with the input sentence\n",
    "        prompt_template = prompt_template.replace(\"[INP]\", row[\"sentence\"])\n",
    "        \n",
    "        answer = unmasker(prompt_template)[0]\n",
    "        answers.append((str(answer[\"score\"]), str(answer[\"token_str\"])))\n",
    "\n",
    "    return {\"answers\": answers}\n",
    "\n",
    "\n",
    "probed_data = data[\"train\"].map(apply_promts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_counts = {}\n",
    "\n",
    "for answer in probed_data[\"answers\"]:\n",
    "    for template, answer in zip(prompt_templates, answer):\n",
    "        if template not in answer_counts:\n",
    "            answer_counts[template] = {}\n",
    "        \n",
    "        if answer[1] not in answer_counts[template]:\n",
    "            answer_counts[template][answer[1]] = 0\n",
    "        \n",
    "        answer_counts[template][answer[1]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_answers(answer_counts, cutoff=10):\n",
    "    filtered_answers = {key: {} for key in answer_counts}\n",
    "\n",
    "    for prompt, answers in answer_counts.items():\n",
    "        for question, count in answers.items():\n",
    "            if count > cutoff:\n",
    "                filtered_answers[prompt][question] = count\n",
    "\n",
    "            else:\n",
    "                if \"_other\" not in filtered_answers[prompt]:\n",
    "                    filtered_answers[prompt][\"_other\"] = count\n",
    "                else:\n",
    "                    filtered_answers[prompt][\"_other\"] += count\n",
    "\n",
    "    return filtered_answers\n",
    "\n",
    "filtered_answer = get_possible_answers(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[INP]. This should be [MASK] knowlege': {'called': 7010,\n",
       "  'david': 160,\n",
       "  'considered': 860,\n",
       "  'a': 436,\n",
       "  'termed': 221,\n",
       "  'my': 116,\n",
       "  '_other': 325,\n",
       "  'your': 91,\n",
       "  'our': 116,\n",
       "  'miss': 41,\n",
       "  'his': 11,\n",
       "  'mr': 48,\n",
       "  'read': 25,\n",
       "  'mrs': 11,\n",
       "  'cameron': 11,\n",
       "  'steve': 12,\n",
       "  'putin': 11},\n",
       " '[INP]. This sentence contains [MASK] proof and results.': {'simple': 849,\n",
       "  'ample': 578,\n",
       "  'no': 3205,\n",
       "  'insufficient': 1928,\n",
       "  'sufficient': 1181,\n",
       "  'both': 406,\n",
       "  'contradictory': 282,\n",
       "  'detailed': 523,\n",
       "  'some': 23,\n",
       "  'false': 111,\n",
       "  'numerous': 152,\n",
       "  'more': 18,\n",
       "  'important': 15,\n",
       "  'remarkable': 33,\n",
       "  '_other': 47,\n",
       "  'explicit': 39,\n",
       "  'excellent': 37,\n",
       "  'additional': 17,\n",
       "  'incorrect': 19,\n",
       "  'surprising': 25,\n",
       "  'many': 17},\n",
       " '[INP]. This sentence contains a statement from a expert, authority, witness, group, organisation or similar? [MASK].': {'etc': 695,\n",
       "  ')': 3711,\n",
       "  'witness': 2078,\n",
       "  'member': 285,\n",
       "  'person': 1309,\n",
       "  'participant': 626,\n",
       "  'expert': 242,\n",
       "  'statement': 15,\n",
       "  'prisoner': 18,\n",
       "  '\"': 282,\n",
       "  '_other': 74,\n",
       "  'employee': 37,\n",
       "  '.': 34,\n",
       "  'question': 18,\n",
       "  'speaker': 38,\n",
       "  'â€¦': 43},\n",
       " '[INP]. This is something I have [MASK] from personal experience.': {'learned': 9076,\n",
       "  'learnt': 429},\n",
       " '[INP]. That is a cool [MASK].': {'thing': 3905,\n",
       "  '_other': 392,\n",
       "  'statement': 493,\n",
       "  'idea': 989,\n",
       "  'option': 76,\n",
       "  'climate': 131,\n",
       "  'day': 463,\n",
       "  'question': 1381,\n",
       "  'sign': 57,\n",
       "  'change': 76,\n",
       "  '##idge': 26,\n",
       "  'assumption': 44,\n",
       "  'prospect': 166,\n",
       "  'rule': 242,\n",
       "  'breeze': 26,\n",
       "  'factor': 60,\n",
       "  'book': 55,\n",
       "  'place': 38,\n",
       "  'assessment': 48,\n",
       "  'trend': 73,\n",
       "  'word': 86,\n",
       "  'lesson': 151,\n",
       "  'record': 13,\n",
       "  'one': 93,\n",
       "  'phrase': 30,\n",
       "  'step': 47,\n",
       "  'year': 11,\n",
       "  'shot': 11,\n",
       "  'strategy': 25,\n",
       "  'trick': 13,\n",
       "  'moment': 13,\n",
       "  'move': 94,\n",
       "  'topic': 13,\n",
       "  'approach': 27,\n",
       "  'decision': 14,\n",
       "  'choice': 11,\n",
       "  'scenario': 11,\n",
       "  'proposition': 24,\n",
       "  'attitude': 18,\n",
       "  'guy': 20,\n",
       "  'story': 12,\n",
       "  'possibility': 12,\n",
       "  'policy': 15}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(answers, filtered_answer):\n",
    "    data  = []\n",
    "\n",
    "    for answer in answers:\n",
    "        sub_result = []\n",
    "        for prompt, answer in zip(prompt_templates, answer):\n",
    "            if answer[1] in filtered_answer[prompt]:\n",
    "                sub_result.append(answer[1])\n",
    "            else:\n",
    "                sub_result.append(\"_other\")\n",
    "\n",
    "        data.append(sub_result)\n",
    "\n",
    "    return data\n",
    "\n",
    "features = get_feature_vector(probed_data[\"answers\"], filtered_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoded_features = encoder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(encoded_features, probed_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eae81d9b2294370828c27c2bafe38c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2716 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation = data[\"validation\"].map(apply_promts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     anecdote       0.40      0.22      0.28       458\n",
      "   assumption       0.73      0.91      0.81      1826\n",
      "common-ground       0.00      0.00      0.00        66\n",
      "        other       0.25      0.03      0.06        30\n",
      "   statistics       0.45      0.15      0.23        66\n",
      "    testimony       0.62      0.39      0.48       219\n",
      "        title       0.73      0.31      0.44        51\n",
      "\n",
      "     accuracy                           0.69      2716\n",
      "    macro avg       0.45      0.29      0.33      2716\n",
      " weighted avg       0.64      0.69      0.65      2716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_features = get_feature_vector(validation[\"answers\"], filtered_answer)\n",
    "validation_encoded_features = encoder.transform(validation_features)\n",
    "\n",
    "pred = rfc.predict(validation_encoded_features)\n",
    "\n",
    "print(classification_report(validation[\"label\"], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing probed answers to disk to experiment with classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "validation_pkl = (validation_features, validation[\"label\"])\n",
    "\n",
    "with open(\"../data/probed/validation_filtered.pkl\", \"wb\") as f:\n",
    "    pickle.dump(validation_pkl, f)\n",
    "\n",
    "validation_raw_answers = validation[\"answers\"]\n",
    "\n",
    "with open(\"../data/probed/validation_raw_answers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(validation_raw_answers, f)\n",
    "\n",
    "train_pkl = (features, probed_data[\"label\"])\n",
    "\n",
    "with open(\"../data/probed/train_filtered.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_pkl, f)\n",
    "\n",
    "train_raw_answers = probed_data[\"answers\"]\n",
    "\n",
    "with open(\"../data/probed/train_raw_answers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_raw_answers, f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef0baf6d8be2d5e36d7cf9c55cfbad6339e5c1a472d19bff39d17af1061bd70a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ltp-project-JPCASTvD-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
